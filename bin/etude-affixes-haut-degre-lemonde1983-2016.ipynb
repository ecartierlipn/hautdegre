{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude des préfixes exprimant le haut degré dans Le Monde (1985-2019)\n",
    "\n",
    "liste des préfixes : ultra-, super-, supra-, hyper-, hypra-, sur-, extra-, méga-, giga-, archi-, maxi-\n",
    "\n",
    "Dans ce travail, nous étudions d'abord les unigrammes (ie formes lexicales composées de l'un des préfixes), puis les bi-grammes (formes PREF-LEXIE ou PREF LEXIE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys, csv,re, random, glob,os\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "import matplotlib\n",
    "import seaborn as sns                            # more plots\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des stoplists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoplist chargée : 50 mots.\n"
     ]
    }
   ],
   "source": [
    "stoplist={}\n",
    "files = glob.glob(\"./stoplists/*.txt\")\n",
    "for file in files:\n",
    "    with open(file, mode=\"r\",encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if len(line.strip())>0:\n",
    "                stoplist[line.strip()]=1\n",
    "            \n",
    "print(\"Stoplist chargée : \" + str(len(stoplist)) + \" mots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des formations préfixées avec annotations\n",
    "\n",
    "format :\n",
    "\n",
    "pref tab year tab count\n",
    "\n",
    "superclasse/NOM\t2017\t1\n",
    "\n",
    "supraterrestre/Adj\t2017\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Ngram big file ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10451 entries, 0 to 10450\n",
      "Data columns (total 3 columns):\n",
      "pref     10451 non-null object\n",
      "year     10451 non-null int64\n",
      "count    10451 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 245.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pref</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>superclasse/NOM</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>supraterrestre/Adj</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperémotivité/Noun</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extraterrestre/Noun</td>\n",
       "      <td>2017</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyperémotivité/NOM</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pref  year  count\n",
       "0      superclasse/NOM  2017      1\n",
       "1   supraterrestre/Adj  2017      1\n",
       "2  hyperémotivité/Noun  2017      2\n",
       "3  extraterrestre/Noun  2017    310\n",
       "4   hyperémotivité/NOM  2017      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if os.path.isfile('../../lemonde1994-2006/prefixes_all.csv'):\n",
    "    print(\"Loading the Ngram big file ...\")\n",
    "    df= pd.read_csv('../../lemonde1994-2006/prefixes_all.csv',  header=0, sep=\"\\t\", error_bad_lines=False)#dtype={'pref':str,'year':datetime64[ns],'count': int64},, index_col=0 \n",
    "else:\n",
    "    print(\"Please first launch X.py to prefixes_all.csv\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# format standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4648 entries, archi-Khanabad/NAM to ultraïste/NOM\n",
      "Data columns (total 15 columns):\n",
      "1987    765 non-null float64\n",
      "1988    728 non-null float64\n",
      "1989    817 non-null float64\n",
      "1990    734 non-null float64\n",
      "1991    730 non-null float64\n",
      "1992    785 non-null float64\n",
      "1994    777 non-null float64\n",
      "2002    1002 non-null float64\n",
      "2003    740 non-null float64\n",
      "2004    839 non-null float64\n",
      "2005    702 non-null float64\n",
      "2006    630 non-null float64\n",
      "2016    408 non-null float64\n",
      "2017    399 non-null float64\n",
      "2018    395 non-null float64\n",
      "dtypes: float64(15)\n",
      "memory usage: 581.0+ KB\n",
      "None\n",
      "year                1987  1988  1989  1990  1991  1992  1994  2002  2003  \\\n",
      "pref                                                                       \n",
      "archi-Khanabad/NAM   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "archi-Satan/NOM      NaN   NaN   1.0   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "archi-Trading/NAM    NaN   NaN   1.0   1.0   NaN   NaN   NaN   NaN   NaN   \n",
      "archi-battu/VER      1.0   1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "archi-bondé/VER      1.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "year                2004  2005  2006  2016  2017  2018  \n",
      "pref                                                    \n",
      "archi-Khanabad/NAM   NaN   2.0   NaN   NaN   NaN   NaN  \n",
      "archi-Satan/NOM      NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "archi-Trading/NAM    NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "archi-battu/VER      NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "archi-bondé/VER      NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#table = pivot_table(df, values='count', index=['string', 'prefix', 'wordpart','word'], columns=['year'], aggfunc=np.sum)\n",
    "df = df.pivot_table(values='count', index=['pref'], columns=['year'], aggfunc=np.sum)\n",
    "#print(table)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(type(df))\n",
    "df = df.fillna(0)\n",
    "df.to_csv(\"df_lemonde_pivot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extraction composants d'information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4004 entries, 3 to 4647\n",
      "Data columns (total 21 columns):\n",
      "string      4004 non-null object\n",
      "1987        4004 non-null float64\n",
      "1988        4004 non-null float64\n",
      "1989        4004 non-null float64\n",
      "1990        4004 non-null float64\n",
      "1991        4004 non-null float64\n",
      "1992        4004 non-null float64\n",
      "1994        4004 non-null float64\n",
      "2002        4004 non-null float64\n",
      "2003        4004 non-null float64\n",
      "2004        4004 non-null float64\n",
      "2005        4004 non-null float64\n",
      "2006        4004 non-null float64\n",
      "2016        4004 non-null float64\n",
      "2017        4004 non-null float64\n",
      "2018        4004 non-null float64\n",
      "prefix      4004 non-null object\n",
      "word        4004 non-null object\n",
      "wordpart    4004 non-null object\n",
      "sep         2091 non-null object\n",
      "pos         4004 non-null object\n",
      "dtypes: float64(15), object(6)\n",
      "memory usage: 688.2+ KB\n",
      "None\n",
      "year              string  1987  1988  1989  1990  1991  1992  1994  2002  \\\n",
      "3        archi-battu/VER   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4        archi-bondé/VER   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "5      archi-bondées/ADJ   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "6       archi-bourré/VER   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
      "7     archi-bruyante/ADJ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "year  2003  ...  2005  2006  2016  2017  2018  prefix            word  \\\n",
      "3      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi     archi-battu   \n",
      "4      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi     archi-bondé   \n",
      "5      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi   archi-bondées   \n",
      "6      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi    archi-bourré   \n",
      "7      1.0  ...   0.0   0.0   0.0   0.0   0.0   archi  archi-bruyante   \n",
      "\n",
      "year  wordpart sep  pos  \n",
      "3        battu   -  VER  \n",
      "4        bondé   -  VER  \n",
      "5      bondées   -  ADJ  \n",
      "6       bourré   -  VER  \n",
      "7     bruyante   -  ADJ  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Empty DataFrame\n",
      "Columns: [string, 1987, 1988, 1989, 1990, 1991, 1992, 1994, 2002, 2003, 2004, 2005, 2006, 2016, 2017, 2018, prefix, word, wordpart, sep, pos]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4004 entries, 3 to 4647\n",
      "Data columns (total 21 columns):\n",
      "string      4004 non-null object\n",
      "1987        4004 non-null float64\n",
      "1988        4004 non-null float64\n",
      "1989        4004 non-null float64\n",
      "1990        4004 non-null float64\n",
      "1991        4004 non-null float64\n",
      "1992        4004 non-null float64\n",
      "1994        4004 non-null float64\n",
      "2002        4004 non-null float64\n",
      "2003        4004 non-null float64\n",
      "2004        4004 non-null float64\n",
      "2005        4004 non-null float64\n",
      "2006        4004 non-null float64\n",
      "2016        4004 non-null float64\n",
      "2017        4004 non-null float64\n",
      "2018        4004 non-null float64\n",
      "prefix      4004 non-null object\n",
      "word        4004 non-null object\n",
      "wordpart    4004 non-null object\n",
      "sep         4004 non-null object\n",
      "pos         4004 non-null object\n",
      "dtypes: float64(15), object(6)\n",
      "memory usage: 688.2+ KB\n",
      "None\n",
      "year                string  1987  1988  1989  1990  1991  1992  1994  2002  \\\n",
      "3          archi-battu/VER   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4          archi-bondé/VER   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "5        archi-bondées/ADJ   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "6         archi-bourré/VER   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
      "7       archi-bruyante/ADJ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "8     archi-chancelier/NOM   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   \n",
      "9      archi-classique/ADJ   0.0   1.0   2.0   0.0   0.0   0.0   1.0   0.0   \n",
      "10    archi-classiques/NOM   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   \n",
      "11       archi-classés/VER   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   \n",
      "12        archi-comble/ADJ   6.0   1.0   3.0   5.0   0.0   1.0   1.0   3.0   \n",
      "\n",
      "year  2003  ...  2005  2006  2016  2017  2018  prefix              word  \\\n",
      "3      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi       archi-battu   \n",
      "4      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi       archi-bondé   \n",
      "5      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi     archi-bondées   \n",
      "6      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi      archi-bourré   \n",
      "7      1.0  ...   0.0   0.0   0.0   0.0   0.0   archi    archi-bruyante   \n",
      "8      0.0  ...   0.0   0.0   0.0   0.0   0.0   archi  archi-chancelier   \n",
      "9      1.0  ...   0.0   0.0   0.0   0.0   0.0   archi   archi-classique   \n",
      "10     0.0  ...   0.0   0.0   0.0   0.0   0.0   archi  archi-classiques   \n",
      "11     0.0  ...   0.0   0.0   0.0   0.0   0.0   archi     archi-classés   \n",
      "12     4.0  ...   0.0   2.0   0.0   0.0   0.0   archi      archi-comble   \n",
      "\n",
      "year    wordpart sep  pos  \n",
      "3          battu   -  VER  \n",
      "4          bondé   -  VER  \n",
      "5        bondées   -  ADJ  \n",
      "6         bourré   -  VER  \n",
      "7       bruyante   -  ADJ  \n",
      "8     chancelier   -  NOM  \n",
      "9      classique   -  ADJ  \n",
      "10    classiques   -  NOM  \n",
      "11       classés   -  VER  \n",
      "12        comble   -  ADJ  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "df = df.reset_index()\n",
    "#print(df.head())\n",
    "pref_re = '^(ultra|sur|super|hyper|hypra|extra|méga|archi|maxi|supra)(?:-)?(?:.*)\\/(?:.+)$'\n",
    "pref_re1 = '^(.+)\\/(?:.+)$'\n",
    "pref_re2 = '^(?:ultra|sur|super|hyper|hypra|extra|méga|archi|maxi|supra)(?:-)?(.*)\\/(?:.+)$'\n",
    "pref_re3 = '^(?:ultra|sur|super|hyper|hypra|extra|méga|archi|maxi|supra)(-)?(?:.*)\\/(?:.+)$'\n",
    "pref_re4 = '^(?:ultra|sur|super|hyper|hypra|extra|méga|archi|maxi|supra)(?:-)?(?:.*)\\/(.+)$'\n",
    "df['prefix'] = df.pref.str.extract(pref_re, expand=True)\n",
    "df['word'] = df.pref.str.extract(pref_re1, expand=True)\n",
    "df['wordpart'] = df.pref.str.extract(pref_re2, expand=True)\n",
    "df['sep'] = df.pref.str.extract(pref_re3, expand=True)\n",
    "df['pos'] = df.pref.str.extract(pref_re4, expand=True)\n",
    "# élimination stoplist\n",
    "#print(df.word.isin(stoplist).count())\n",
    "df = df[~df.word.isin(stoplist)]\n",
    "#print(df.wordpart.str.contains(\"^[A-Z]\").count())\n",
    "df = df[~df.wordpart.str.contains(\"^[A-Z]\")]\n",
    "df.rename(index=str, columns={'pref':'string'}, inplace=True)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "# Nan Values?\n",
    "print(df[df.prefix.isna()])\n",
    "df = df.fillna('')\n",
    "#df.dropna(inplace=True)\n",
    "print(df.info())\n",
    "print(df.head(10))\n",
    "\n",
    "df.to_csv(\"df_lemonde.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4004 entries, 3 to 4647\n",
      "Data columns (total 16 columns):\n",
      "1987          4004 non-null float64\n",
      "1988          4004 non-null float64\n",
      "1989          4004 non-null float64\n",
      "1990          4004 non-null float64\n",
      "1991          4004 non-null float64\n",
      "1992          4004 non-null float64\n",
      "1994          4004 non-null float64\n",
      "2002          4004 non-null float64\n",
      "2003          4004 non-null float64\n",
      "2004          4004 non-null float64\n",
      "2005          4004 non-null float64\n",
      "2006          4004 non-null float64\n",
      "2016          4004 non-null float64\n",
      "2017          4004 non-null float64\n",
      "2018          4004 non-null float64\n",
      "full_count    0 non-null float64\n",
      "dtypes: float64(16)\n",
      "memory usage: 531.8+ KB\n",
      "None\n",
      "year              string  1987  1988  1989  1990  1991  1992  1994  2002  \\\n",
      "3        archi-battu/VER   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4        archi-bondé/VER   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "5      archi-bondées/ADJ   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "6       archi-bourré/VER   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
      "7     archi-bruyante/ADJ   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "year  2003  ...  2006  2016  2017  2018  prefix            word  wordpart sep  \\\n",
      "3      0.0  ...   0.0   0.0   0.0   0.0   archi     archi-battu     battu   -   \n",
      "4      0.0  ...   0.0   0.0   0.0   0.0   archi     archi-bondé     bondé   -   \n",
      "5      0.0  ...   0.0   0.0   0.0   0.0   archi   archi-bondées   bondées   -   \n",
      "6      0.0  ...   0.0   0.0   0.0   0.0   archi    archi-bourré    bourré   -   \n",
      "7      1.0  ...   0.0   0.0   0.0   0.0   archi  archi-bruyante  bruyante   -   \n",
      "\n",
      "year  pos full_count  \n",
      "3     VER        NaN  \n",
      "4     VER        NaN  \n",
      "5     ADJ        NaN  \n",
      "6     VER        NaN  \n",
      "7     ADJ        NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(df.info())\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "newdf = df.select_dtypes(include=numerics)\n",
    "print(newdf.info())\n",
    "#df['full_count'] = df.select_dtypes(include=numerics).sum(numeric_only=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataframes with relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yearly corpus stats (total words)\n",
    "from datetime import datetime\n",
    "\n",
    "def load_lemonde_counts(fn):\n",
    "    totals={}\n",
    "    with open(fn, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(\"\\t\")\n",
    "            if len(data)==2 and not(data[0]== 'year'):\n",
    "                year = datetime.strptime(data[0], '%Y')\n",
    "                totals[year.year]=int(data[1])\n",
    "    return totals\n",
    "\n",
    "# load totals of tokens per corpus year\n",
    "totals = load_lemonde_counts(\"../../lemonde1994-2006/frequency.csv\")\n",
    "#print(totals)\n",
    "dfcount = pd.DataFrame.from_dict(totals, orient='index')\n",
    "#dfcount.index.sort_values()\n",
    "#print(dfcount)\n",
    "dfcount.sort_index().plot(kind=\"bar\", title=\"Evolution de la taille des corpus Le Monde\")\n",
    "#pdf0.savefig()\n",
    "#plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now create rel version for dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel = df.copy(deep=True)\n",
    "df_rel= df_rel.fillna(0)\n",
    "#print(df_rel.head())\n",
    "#print(df_rel.info())\n",
    "#print(df_rel.columns)\n",
    "# calculate relative frequency for each column\n",
    "\n",
    "for i in totals.keys():\n",
    "    #print(i,type(i),df_rel[i],totals[i])\n",
    "    df_rel[str(i) + '_freqrel'] = (df_rel[i] / totals[i]) * 1000\n",
    "\n",
    "# remove absolute frequency for df (relative frequency used for clustering and plotting)\n",
    "df_rel = df_rel.drop([i for i in totals.keys()], axis=1)\n",
    "df_rel.columns = df_rel.columns.str.replace('_freqrel', \"\")\n",
    "df_rel = df_rel.infer_objects()\n",
    "print(df_rel.info())\n",
    "print(df_rel.head(10))\n",
    "\n",
    "df_rel.to_csv(\"dfrel_lemonde.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération synthèse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for moving average\n",
    "# definition of plot for all measures\n",
    "def plot_rolling(df,title, window=10):\n",
    "    fig, ax = plt.subplots(3,figsize=(20, 10))\n",
    "    ax[0].plot(df.index, df.data, label='raw data')\n",
    "    ax[0].plot(df.data.rolling(window=window).mean(), label=\"rolling mean (window=10)\");\n",
    "    ax[0].plot(df.data.rolling(window=window).std(), label=\"rolling std (window=10)\");\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(df.index, df.z_data, label=\"de-trended data\")\n",
    "    ax[1].plot(df.z_data.rolling(window=window).mean(), label=\"rolling mean (window=10)\");\n",
    "    ax[1].plot(df.z_data.rolling(window=window).std(), label=\"rolling std (window=10)\");\n",
    "    ax[1].legend()\n",
    "\n",
    "    ax[2].plot(df.index, df.zp_data, label=\"5 lag differenced de-trended data\")\n",
    "    ax[2].plot(df.zp_data.rolling(window=window).mean(), label=\"rolling mean (window=10)\");\n",
    "    ax[2].plot(df.zp_data.rolling(window=window).std(), label=\"rolling std (window=10)\");\n",
    "    ax[2].legend()\n",
    "    fig.suptitle(title, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    fig.autofmt_xdate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.select_dtypes(include=[np.number]).columns))\n",
    "print(list(df_rel.select_dtypes(include=[np.number]).columns))\n",
    "periods = list(df.select_dtypes(include=[np.number]).columns)\n",
    "periods2 = [str(i) for i in periods]\n",
    "#periods = [int(elt) for elt in df.columns if int(elt)]\n",
    "#print(df[periods].head())\n",
    "#print(df_rel[periods].head())\n",
    "# create a PdfPages object\n",
    "pdf0 = PdfPages('Prefix_synthesis_lemonde.pdf')\n",
    "\n",
    "group_data = df.groupby('prefix').sum()\n",
    "group_data2 = df_rel.groupby('prefix').sum()\n",
    "fig, ax = plt.subplots(3, figsize=(45, 30))\n",
    "group_data2[periods2].T.plot(ax=ax[0],rot=45,figsize=(15,10),title=\"Evolution des fréquences relatives des préfixes (tous) entre 1800 et 2010\")\n",
    "ax[0].set_xticklabels([])\n",
    "group_data[periods].T.plot(ax=ax[1],rot=45,figsize=(15,10),title=\"Evolution des fréquences absolues des préfixes (tous) entre 1800 et 2010\")\n",
    "group_data['full_count'] = group_data.apply(lambda x: x.sum(), axis=1)\n",
    "group_data['full_count'].plot(ax=ax[2],kind=\"bar\",rot=45,figsize=(15,10),title=\"Distribution des fréquences totales entre préfixes\")\n",
    "plt.tight_layout()\n",
    "pdf0.savefig()\n",
    "plt.close()\n",
    "\n",
    "# moving average, trends etc.\n",
    "df5 = group_data2[periods2]\n",
    "for pref in df5.index.values:\n",
    "    series = df5.loc[pref]\n",
    "    ts = pd.DataFrame({'data':series.values}, index=series.index) # 'year':seriesrel.index, \n",
    "\n",
    "    ts['z_data'] = (ts['data'] - ts.data.rolling(window=3).mean()) / ts.data.rolling(window=3).std()\n",
    "    ts['zp_data'] = ts['z_data'] - ts['z_data'].shift(3)\n",
    "    plot_rolling(ts,title= pref, window=10)\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "\n",
    "# distribution by word for each prefix\n",
    "#df['full_count'] = df[periods].sum() # \n",
    "\n",
    "df['full_count'] = df.apply(lambda x: x.sum(), axis=1) # [periods]\n",
    "#print(df['full_count'])\n",
    "for pref in df.prefix.unique():\n",
    "        fig, ax = plt.subplots(2, figsize=(20, 15))\n",
    "        dfplot = df[df.prefix.str.contains(pref)].sort_values('full_count', ascending=False)#['full_count']\n",
    "        # get 0.95 quantile\n",
    "        #print(dfplot)\n",
    "        q = dfplot[\"full_count\"].quantile(0.9)\n",
    "        dfplot2 = dfplot[dfplot[\"full_count\"] < q]['full_count']      \n",
    "        sns.distplot(dfplot2, ax=ax[0]).set_title(\"Distribution des occurrences pour : \" + pref + ' (90% quantiles)')\n",
    "        #sns.distplot(np.log(dfplot), ax=ax[0])\n",
    "        sns.boxplot(dfplot['full_count'],ax=ax[1]).set_title(\"Distribution des occurrences pour : \" + pref + ' (global)')\n",
    "        pdf0.savefig()\n",
    "        plt.close()         \n",
    "\n",
    "        fig, ax = plt.subplots(2, figsize=(20, 15))\n",
    "        dfplot3 = df[df.prefix.str.contains(pref)].set_index('wordpart')[periods].T\n",
    "        dfplot3.index = pd.to_datetime(dfplot3.index)\n",
    "        dfplot4=dfplot3.resample('5AS').sum()\n",
    "        #dfplot4[\"year\"] = dfplot4.index.year\n",
    "        #dfplot4.diff().plot(ax=ax[0], title=pref + \" : accroissement / décroissement des fréquences pour les formations (diff)\")\n",
    "        dfplot4['mean_diff'] = dfplot4.diff().apply(lambda x : x.mean(), axis=1)\n",
    "        dfplot4['mean_diff'].plot(ax=ax[0], title=pref + \" : accroissement / décroissement des fréquences pour les formations (mean of diff)\")\n",
    "        dfplot4['total_distinct_words'] = dfplot4.apply(lambda x : x.astype(bool).sum(), axis=1)\n",
    "        dfplot4['total_distinct_words'].plot(ax=ax[1],title=pref + \" : évolution du nombre de formations lexicales attestées\")\n",
    "        pdf0.savefig()\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        df[df.prefix.str.contains(pref)].sort_values('full_count', ascending=False).set_index('wordpart')[periods].head(50).T.plot(title= pref + \" :  évolution des formations les plus fréquentes (50)\")\n",
    "        pdf0.savefig()\n",
    "        plt.close()\n",
    "\n",
    "pdf0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe df4\n",
    "# save to excel\n",
    "def save_report(report, key, outfile):\n",
    "    \"\"\"\n",
    "    Take a report and save it to a single Excel file\n",
    "    \"\"\"\n",
    "    #cols_tmp = [str(i) for i in report.columns.values]\n",
    "    #cols = sorted(cols_tmp,reverse=True)\n",
    "    #print(cols)\n",
    "    writer = pd.ExcelWriter(outfile)\n",
    "    for k, grp in report.groupby(key):\n",
    "        grp.sort_values('full_count', ascending=False).set_index('wordpart').to_excel(writer,k)\n",
    "    writer.save()\n",
    "    return True\n",
    "\n",
    "#print(df4.head())\n",
    "save_report(df, 'prefix','prefixes_lemonde1994-2018.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as pytools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "#pytools.set_credentials_file(username='ecartierdijon', api_key='3msHhM6RjRcAvIpAgcz6')\n",
    "#help(py.plot) max 25 public graphs\n",
    "\n",
    "# data\n",
    "#print(df4_rel.head())\n",
    "group_data = df_rel.groupby('prefix').sum()\n",
    "df6 = group_data[periods2].T\n",
    "print(df6.head())\n",
    "data = []\n",
    "for pref in df6.columns.values:\n",
    "    linegraph = go.Scatter(\n",
    "        x=df6.index,\n",
    "        y=df6[pref],\n",
    "        name = pref,\n",
    "        #line = dict(color = '#17BECF'),\n",
    "        opacity = 0.8)\n",
    "    data.append(linegraph)\n",
    "\n",
    "layout = dict(\n",
    "    title='Evolution des fréquences relatives de 1800 à 2010',\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label='1m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=6,\n",
    "                     label='6m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "plot(fig, filename = \"./plotly_graphs/Evolution_relative-prefixes.html\") \n",
    "iplot(fig) # , filename = \"Time Series with Rangeslider\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as pytools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "#pytools.set_credentials_file(username='ecartierdijon', api_key='3msHhM6RjRcAvIpAgcz6')\n",
    "#help(py.plot) max 25 public graphs\n",
    "\n",
    "# data\n",
    "#print(df4_rel.head())\n",
    "group_data = df.groupby('prefix').sum()\n",
    "df6 = group_data[periods].T\n",
    "print(df6.head())\n",
    "data = []\n",
    "for pref in df6.columns.values:\n",
    "    linegraph = go.Scatter(\n",
    "        x=df6.index,\n",
    "        y=df6[pref],\n",
    "        name = pref,\n",
    "        #line = dict(color = '#17BECF'),\n",
    "        opacity = 0.8)\n",
    "    data.append(linegraph)\n",
    "\n",
    "layout = dict(\n",
    "    title='Evolution des fréquences relatives de 1800 à 2010',\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label='1m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=6,\n",
    "                     label='6m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "plot(fig, filename = \"./plotly_graphs/Evolution_absolue-prefixes.html\") \n",
    "iplot(fig) # , filename = \"Time Series with Rangeslider\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with plotly -  for more flexibility, use Dash!\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as pytools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "#pytools.set_credentials_file(username='ecartierdijon', api_key='3msHhM6RjRcAvIpAgcz6')\n",
    "#help(py.plot) max 25 public graphs\n",
    "\n",
    "figs = {}\n",
    "prefs = ('ultra','super','hyper','hypra','extra','méga','archi','maxi','supra')\n",
    "# data\n",
    "i = 0\n",
    "for pref in df.prefix.unique():\n",
    "    if df[df.prefix.str.contains(pref)].wordpart.count() > 0:\n",
    "        i = i + 1\n",
    "        dfplot = df[df.prefix.str.contains(pref)].sort_values('full_count', ascending=False).set_index('wordpart') # .head(30)\n",
    "        total_words = len(dfplot.index)\n",
    "        total_occ = dfplot['full_count'].sum()\n",
    "        dfplotT =  dfplot[periods].T\n",
    "        #plot(title= pref + \" :  évolution des formations les plus fréquentes (30)\")\n",
    "        #print(dfplot)\n",
    "        data = []\n",
    "        for word in dfplotT.columns.values:\n",
    "            linegraph = go.Scatter(\n",
    "                x=dfplotT.index,\n",
    "                y=dfplotT[word],\n",
    "                name = word,\n",
    "                #line = dict(color = '#17BECF'),\n",
    "                opacity = 0.8)\n",
    "            #fig.append_trace(linegraph, i, 1)\n",
    "            data.append(linegraph)\n",
    "\n",
    "        layout = dict(\n",
    "            title= pref + \" : évolution des fréquences relatives de 1800 à 2010 (\"  + str(total_words) + ' mots distincts, ' + str(int(total_occ)) + ' occurrences)',\n",
    "            xaxis=dict(\n",
    "                rangeselector=dict(\n",
    "                    buttons=list([\n",
    "                        dict(count=1,\n",
    "                             label='1m',\n",
    "                             step='month',\n",
    "                             stepmode='backward'),\n",
    "                        dict(count=6,\n",
    "                             label='6m',\n",
    "                             step='month',\n",
    "                             stepmode='backward'),\n",
    "                        dict(step='all')\n",
    "                    ])\n",
    "                ),\n",
    "                rangeslider=dict(\n",
    "                    visible = True\n",
    "                ),\n",
    "                type='date'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #fig.append_trace(dict(data=data, layout=layout), i, 1)\n",
    "        fig = dict(data=data, layout=layout)\n",
    "        figs[pref]=fig\n",
    "        \n",
    "        \n",
    "#   Layout setting\n",
    "#fig['layout'].update(height=1000)\n",
    "\n",
    "#   Create Html\n",
    "#folder_name = 'folder_name' + '.html'\n",
    "#plotly.offline.plot(fig, filename=folder_name)\n",
    "for pref in figs.keys():\n",
    "    plot(figs[pref], filename = \"./plotly_graphs/Evolution-prefixes-\" + pref + \"-words2.html\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get boxplot of frequency for each prefix\n",
    "To be done : evolution of distribution (boxplot) + for every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf0 = PdfPages('Prefix_synthesis2.pdf')\n",
    "# other means to detrend\n",
    "from statsmodels.tsa.tsatools import detrend\n",
    "for pref in df5.index.values:\n",
    "    series = df5.loc[pref]\n",
    "    data = pd.DataFrame({pref:series.values}, index=series.index) # 'year':seriesrel.index, \n",
    "    #print(data)\n",
    "    notrend = detrend(data[pref])\n",
    "    data[\"notrend\"] = notrend\n",
    "    data[\"trend\"] = data[pref] - notrend\n",
    "    data.tail()\n",
    "    data.plot(y=[pref, \"notrend\", \"trend\"], figsize=(20,10), title=pref + \": detrending\")\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "    # On essaye de calculer une tendance en minimisant : Y = α + βt + γt\n",
    "    notrend2 = detrend(data[pref], order=2)\n",
    "    data[\"notrend2\"] = notrend2\n",
    "    data[\"trend2\"] = data[pref] - data[\"notrend2\"]\n",
    "    data.plot(y=[pref, \"notrend2\", \"trend2\"], figsize=(20,10), title=pref + \": detrending and tendency\")\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "    # same with log\n",
    "    import numpy\n",
    "    data[\"logSess\"] = data[pref].apply(lambda x: numpy.log(x+1))\n",
    "    lognotrend = detrend(data['logSess'])\n",
    "    data[\"lognotrend\"] = lognotrend\n",
    "    data[\"logtrend\"] = data[\"logSess\"] - data[\"lognotrend\"]\n",
    "    data.plot(y=[\"logSess\", \"lognotrend\", \"logtrend\"], figsize=(20,10), title=pref + \": detrending and tendency (log)\")\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composante saisonnière\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "for pref in df5.index.values:\n",
    "    series = df5.loc[pref]\n",
    "    data = pd.DataFrame({pref:series.values}, index=series.index) # 'year':seriesrel.index, \n",
    "    #print(data)\n",
    "\n",
    "    res = seasonal_decompose(data[pref].as_matrix().ravel(), freq=7, two_sided=False)\n",
    "    data[\"season\"] = res.seasonal\n",
    "    data[\"trendsea\"] = res.trend\n",
    "    data.plot(y=[pref, \"season\", \"trendsea\"], figsize=(20,10), title=pref + \" : saisonnalité\")\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "    data[-30:].plot(y=[pref, \"season\", \"trendsea\"], figsize=(20,10), title=pref + \" : saisonnalité (30 dernières années)\")\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "#pdf0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import periodogram\n",
    "for pref in df5.index.values:\n",
    "    series = df5.loc[pref]\n",
    "    data = pd.DataFrame({pref:series.values}, index=series.index) # 'year':seriesrel.index, \n",
    "    #print(data)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = plot_acf(data[pref], lags=40, ax=ax1)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = plot_pacf(data[pref], lags=40, ax=ax2)\n",
    "    fig.suptitle(pref + \" : autocorrelation\")\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "    p = periodogram(data[pref])\n",
    "    plt.plot(p)\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "\n",
    "pdf0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose in trend, seasonality and residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "#fig, ax = plt.subplots(1,1,figsize = (20,20))\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "for pref in df5.index.values:\n",
    "    series = df5.loc[pref]\n",
    "    df2 = pd.DataFrame({'data':series.values}, index=series.index) # 'year':seriesrel.index, \n",
    "\n",
    "    #series = [i+randrange(10) for i in range(1,100)]\n",
    "    res = seasonal_decompose(df2, model='additive', freq=1)\n",
    "    res.plot()\n",
    "    pdf0.savefig()\n",
    "    plt.close()\n",
    "    pyplot.show()\n",
    "    #res = seasonal_decompose(df2, model='multiplicative', freq=1)\n",
    "    #res.plot()\n",
    "    #pdf0.savefig()\n",
    "    #plt.close()\n",
    "    #pyplot.show()\n",
    "    #print(result.trend)\n",
    "    #print(result.seasonal)\n",
    "    #print(result.resid)\n",
    "    #print(result.observed)\n",
    "pdf0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate lines where you have POS info (word_POS, prefix_POS, _POS_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get row with pos info in another dataframe\n",
    "filter0 = df.string.str.contains(\"_\")\n",
    "print(len(df[filter0].index))\n",
    "print(\"\\n********************\\n\")\n",
    "filter = df.prefix.str.contains(\"_\")\n",
    "print(len(df[filter].index))\n",
    "print(df.prefix.unique())\n",
    "print(\"\\n********************\\n\")\n",
    "filter2 = df.word.str.contains(\"_\")\n",
    "print(len(df[filter2].index))\n",
    "print(df[filter2].word.unique())\n",
    "print(\"\\n********************\\n\")\n",
    "print(len(df[filter & filter2].index))\n",
    "print(\"\\n********************\\n\")\n",
    "\n",
    "# create dataframes\n",
    "dfpos = df[filter0]\n",
    "dfraw = df[~filter0]\n",
    "print(dfpos.info())\n",
    "print(dfraw.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfpos.info())\n",
    "dfpos[~dfpos.prefix.str.contains('_')].groupby(['prefix'])[['full_count']].sum()\n",
    "#print(dfpos[['string','prefix','word','full_count']].head(100))\n",
    "#print(df.info())\n",
    "# by prefix \n",
    "#dfpos.plot(subplots=True)\n",
    "#sns.boxplot(x=grp.unstack().prefix, y=grp.unstack().full_count, data=grp.unstack())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos.fillna(0)\n",
    "# first means\n",
    "#res=  df['prefix'].str.split('_', n=1, expand=True)\n",
    "#print(res)\n",
    "dfpos[['prefix1','prefix_pos']] = df['prefix'].str.split('_', n=1, expand=True)\n",
    "dfpos[['word1','word_pos']] = df['word'].str.split('_', n=1, expand=True)\n",
    "# create _pos column word word and prefix\n",
    "#dfpos['prefix_pos'] = dfpos['prefix'].str.extract(\"_(.+)$\", expand=False)\n",
    "#dfpos['prefix1'] = dfpos['prefix'].str.extract(\"^(.+?)_\", expand=False)\n",
    "\n",
    "#dfpos[\"word_pos\"]=  dfpos['word'].str.extract(\"_(.+)$\", expand=False)\n",
    "#dfpos[\"word1\"]= dfpos['word'].str.extract(\"^(.+?)_\", expand=False)\n",
    "  \n",
    "dfpos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = PdfPages('Prefix-X_synthesis2.pdf')\n",
    "columns = dfpos.columns\n",
    "periods = [elt for elt in columns if re.match(\"[0-9]{4}\", elt)]\n",
    "#print(periods)\n",
    "dfpos2 = dfpos[~dfpos.prefix.str.contains(\"_\")]# & dfpos.word_pos.str.contains(\"_\")\n",
    "for k,grp in dfpos2.groupby(['prefix']):\n",
    "    grp[grp.word_pos.str.contains(\"_\")].sort_values(['word_pos','full_count'],ascending=False).groupby('word_pos')[periods].sum().T.plot(kind=\"line\", title=k, rot=45, figsize=(20,10))  # [['full_count']]\n",
    "    pdf2.savefig()\n",
    "    plt.close()\n",
    "#    for k2, grp2 in grp[~grp.word_pos.str.contains('_')].groupby('word_pos'):\n",
    "    for k2, grp2 in grp[grp.word_pos.isin(['ADJ','ADV','NOUN','VERB'])].groupby('word_pos'):\n",
    "        grp2.sort_values(['word1','full_count'],ascending=False).groupby(['word1'])[periods].sum().head(20).T.plot(kind=\"line\", title=k + \"-\" + k2, figsize=(20,10) )\n",
    "\n",
    "        #grp2.groupby('word1').head(10)[periods].sum().plot(kind=\"barh\", title=k + \" - \" + k2, rot=45, figsize=(10,5))  # [['full_count']]\n",
    "        pdf2.savefig()\n",
    "        plt.close()\n",
    "        \n",
    "    #print(grp.unstack())\n",
    "    #print(pd.pivot_table(dfpos2[dfpos2.prefix==k], index= ['word_pos','word'])) # , columns=[periods] , values=\n",
    "pdf2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos2 = dfpos[~dfpos.prefix.str.contains(\"_\")]# & dfpos.word_pos.str.contains(\"_\")\n",
    "dfpos2.sort_values(['prefix','full_count'],ascending=False).groupby(['prefix'])[periods].sum().T.plot(kind=\"line\", figsize=(20,10), logy=True)  # , subplots=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate as bokeh dashboard : https://realpython.com/python-data-visualization-bokeh/\n",
    "\"\"\"Bokeh Visualization Template\n",
    "This template is a general outline for turning your data into a \n",
    "visualization using Bokeh.\n",
    "\"\"\"\n",
    "# Data handling\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "from datetime import date\n",
    "from random import randint\n",
    "\n",
    "# Bokeh libraries\n",
    "from bokeh.io import output_file, output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, CustomJS, HoverTool\n",
    "from bokeh.layouts import row, column, gridplot # https://bokeh.pydata.org/en/latest/docs/user_guide/layout.html\n",
    "from bokeh.models.widgets import Div, Tabs, Panel, Select,DataTable, DateFormatter, TableColumn\n",
    "\n",
    "# Determine where the visualization will be rendered\n",
    "output_file('Prefixes_2grams.html')  # Render to static HTML, or \n",
    "output_notebook()  # Render inline in a Jupyter Notebook\n",
    "\n",
    "# Load and Prepare the data : timerseries of every prefix through time\n",
    "dfpos2 = dfpos[~dfpos.prefix.str.contains(\"_\")]# & dfpos.word_pos.str.contains(\"_\")\n",
    "#print(dfpos2.head())\n",
    "dfpos3 = dfpos2.sort_values(['prefix','full_count'],ascending=False).groupby(['prefix'])[periods].sum().T\n",
    "print(dfpos2[['prefix','full_count']])\n",
    "df4 = dfpos3.reset_index() #.drop('prefix', axis=1)\n",
    "#print(df4.head())\n",
    "data = df4.reset_index().drop('level_0', axis=1).to_dict(orient='list')\n",
    "#print(data)\n",
    "source = ColumnDataSource(data)\n",
    "#print(df4.head())\n",
    "\n",
    "# callback functions\n",
    "callback = CustomJS(args=dict(figure=fig2), code=\"\"\"\n",
    "    var f = cb_obj.value\n",
    "    console.log(f)\n",
    "    if (f == 'Log'){\n",
    "    console.log(figure)\n",
    "    figure.y_axis_type=\"log\"}\n",
    "    figure.trigger('change');\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# rendering\n",
    "palette = ['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b','#e377c2','#7f7f7f','#bcbd22','#17becf']\n",
    "TOOLS = 'crosshair,hover,save,pan,box_zoom,reset,wheel_zoom'\n",
    "TOOLTIPS = [\n",
    "    (\"Année\", \"@x\"),\n",
    "    (\"Fréquence\", \"@y\")]\n",
    "\n",
    "# div element at the beginning of the page\n",
    "div = Div(text=\"\"\"<h1>Etude des préfixes exprimant la haute intensité à partir des données de Google Ngrams</h1>\n",
    "<p>Le premier schéma propose les données générales : répartition temporelle des fréquences entre les différents préfixes.</p>\n",
    "<p>Ensuite, vous pouvez voir le détail en choisissant un préfixe et éventuellement des restrictions (partie du discours et mot)</p><hr/><br/><hr/>\n",
    "\n",
    "\"\"\", width=1000, height=100)\n",
    "\n",
    "# general figure (overall distribution between prefixes)\n",
    "fig0= figure(plot_height=300, plot_width=1000,title=\"Distribution des Prefixes (absolute frequency)\", tooltips=TOOLTIPS, tools = TOOLS, toolbar_location='right')\n",
    "i = 0\n",
    "for pref in data.keys():\n",
    "    if pref not in ('index','sur'):\n",
    "        fig0.line(x=data['index'], y=np.log(data[pref]), legend=pref,line_color=palette[i], line_width = 2)\n",
    "        i = i+1\n",
    "\n",
    "# general figure (absolute frequency)\n",
    "fig2 = figure(plot_height=300, plot_width=1000,title=\"Evolution des Prefixes de 1800 à 2010 (absolute frequency)\", tooltips=TOOLTIPS, tools = TOOLS, toolbar_location='right')\n",
    "i = 0\n",
    "for pref in data.keys():\n",
    "    if pref not in ('index','sur'):\n",
    "        fig2.line(x=data['index'], y=data[pref], legend=pref,line_color=palette[i], line_width = 2)\n",
    "        i = i+1\n",
    "fig2.legend.click_policy = 'hide'\n",
    "tab1 = Panel(child=fig2, title=\"Fréquence absolue\")\n",
    "\n",
    "fig1 = figure(plot_height=300, plot_width=1000,title=\"Evolution des Prefixes de 1800 à 2010 (absolute frequency)\", tooltips=TOOLTIPS, tools = TOOLS, toolbar_location='right')\n",
    "i = 0\n",
    "for pref in data.keys():\n",
    "    if pref not in ('index','sur'):\n",
    "        fig1.line(x=data['index'], y=np.log(data[pref]), legend=pref,line_color=palette[i], line_width = 2)\n",
    "        i = i+1\n",
    "fig1.legend.click_policy = 'hide'\n",
    "tab2 = Panel(child=fig1, title=\"Fréquence Log 2\")\n",
    "\n",
    "tabs = Tabs(tabs=[ tab1, tab2 ],width=500)\n",
    "\n",
    "# vbar for each prefix\n",
    "#vbars = []\n",
    "#for pref in data.keys()\n",
    "#figure(x_range=periods, plot_height=250, title=\"Fruit Counts\",\n",
    "#           toolbar_location=None, tools=\"\").vbar(x=fruits, top=counts, width=0.9)\n",
    "\n",
    "#p.xgrid.grid_line_color = None\n",
    "#p.y_range.start = 0\n",
    "\n",
    "\n",
    "# select\n",
    "select0 = Select(title=\"Choissisez un préfixe\", value=\"\", options=list(dfpos3.columns.values), callback=callback)\n",
    "select1 = Select(title=\"Choissisez une forme de mot\", value=\"\", options=[])\n",
    "#select2 = Select(title=\"Choissisez une mesure\", value=\"\", options=[\"Fréquence absolue\", \"Fréquence relative\", \"Log\"])\n",
    "#select2.js_on_change('value', callback)\n",
    "\n",
    "\n",
    "columns = []\n",
    "for col in df4.columns.values:\n",
    "    columns.append(TableColumn(field=col, title=col))\n",
    "datatable = DataTable(source=source, columns=columns, width=1000, height=280, editable=True)\n",
    "\n",
    "\n",
    "grid = gridplot([ [div],[tabs],[select0, select1, None], [datatable, None, None]])\n",
    "show(grid)\n",
    "# Preview and save \n",
    "#show(fig)  # See what I made, and save if I like it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in ('1800','1850','1900','1950','2000'):\n",
    "    #dfpos[dfpos.prefix=='extra'].groupby(['word_pos','word1'])[year].sum().head(25).T.plot(kind=\"barh\", title=year) \n",
    "    #plt.show()\n",
    "    for k, grp in dfpos[dfpos.prefix=='extra'].groupby(['word_pos']):\n",
    "        if re.search(r\"_$\", k) == None:\n",
    "            for year in ('1800','1850','1900','1950','2000'):\n",
    "                grp.sort_values(['word1','full_count'],ascending=False).groupby(['word1'])[year].sum().head(20).plot(kind=\"barh\", title=year + \"-\" + k )\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD  : generate a bokeh interactive html dashboard with :\n",
    "- list of prefix / list of words\n",
    "- distribution by year for word_pos, word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpos.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot timeline for each prefix\n",
    "df2 = df.drop(['word','string'],axis=1)\n",
    "\n",
    "group_data = df2.groupby('prefix').sum()\n",
    "\n",
    "#print(group_data)\n",
    "group_data.T.plot(subplots=True,rot=45,figsize=(10,5),title=\"Evolution des fréquences totales des préfixes entre 1800 et 2010\")\n",
    "group_data.T.plot(logy=True,rot=45,figsize=(10,5),title=\"Evolution des fréquences totales des préfixes entre 1800 et 2010 (Log Y)\")\n",
    "group_data.T.plot(rot=45,figsize=(10,5),title=\"Evolution des fréquences totales des préfixes entre 1800 et 2010\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_re = '^(ultra|super|hyper|hypra|extra|méga|archi|maxi|supra)\\s+_.+?_$'\n",
    "df2 = df[df.string.str.contains(pref_re, regex=\"True\", case=False)]\n",
    "print(df2.info())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
